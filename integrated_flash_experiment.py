#!/usr/bin/env python3
"""
üöÄ EXP√âRIENCE FLASH ATTENTION AVEC MONITORING COMPLET
Comparaison scientifique exhaustive Flash Attention vs SDPA
D√©velopp√© par Kennedy Kitoko üá®üá©

NOUVEAU: Monitoring complet de TOUTES les m√©triques syst√®me
"""

import os
import sys
import time
import json
import subprocess
import torch
import psutil
from datetime import datetime
from ultralytics import YOLO

# Import du syst√®me de monitoring complet
from comprehensive_monitor import ComprehensiveMonitor

class AdvancedFlashAttentionExperiment:
    """
    Exp√©rience Flash Attention avec monitoring exhaustif
    """
    
    def __init__(self):
        self.experiment_name = f"flash_attention_complete_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        self.results_dir = f'runs/flash_comparison/{self.experiment_name}'
        self.setup_log = []
        self.installation_success = False
        
        # üî¨ NOUVEAU: Initialisation monitoring complet
        self.monitor = ComprehensiveMonitor("FLASH_ATTENTION", self.results_dir)
        
        os.makedirs(self.results_dir, exist_ok=True)
        
        print(f"üî¨ Exp√©rience Flash Attention avec monitoring complet initialis√©e")
        print(f"üìÅ Dossier: {self.results_dir}")
        
    def verify_system_compatibility(self):
        """V√©rification compatibilit√© avec capture environnement"""
        print("üîç V√âRIFICATION COMPATIBILIT√â + CAPTURE ENVIRONNEMENT")
        print("=" * 70)
        
        # üî¨ NOUVEAU: Capture environnement complet AVANT test
        env_data = self.monitor.capture_complete_environment()
        
        compatibility_report = {
            'cuda_compatible': False,
            'pytorch_compatible': False,
            'gpu_compatible': False,
            'system_ready': False,
            'environment_captured': True,
            'environment_data': env_data
        }
        
        # V√©rifications compatibilit√© (code existant)
        from packaging import version

        if torch.cuda.is_available():
            cuda_version = torch.version.cuda
            print(f"‚úÖ CUDA Version: {cuda_version}")

            if version.parse(cuda_version) >= version.parse("11.8"):
                compatibility_report['cuda_compatible'] = True
                print(f"‚úÖ CUDA {cuda_version} Compatible (>=11.8)")
            else:
                print(f"‚ùå CUDA {cuda_version} < 11.8 Required")

        pytorch_version = torch.__version__
        pytorch_major = int(pytorch_version.split('.')[0])
        pytorch_minor = int(pytorch_version.split('.')[1])
        
        print(f"üî• PyTorch Version: {pytorch_version}")
        
        if pytorch_major >= 2 and pytorch_minor >= 2:
            compatibility_report['pytorch_compatible'] = True
            print("‚úÖ PyTorch 2.2+ Compatible")
        else:
            print(f"‚ùå PyTorch {pytorch_version} < 2.2 Required")
        
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            print(f"üéÆ GPU: {gpu_name}")
            
            # Check GPU compatibility selon documentation
            ampere_gpus = ['RTX 30', 'RTX 40', 'A100', 'A40', 'A30']
            ada_gpus = ['RTX 40']
            hopper_gpus = ['H100', 'H200']
            
            is_compatible = any(gpu_type in gpu_name for gpu_type in ampere_gpus + ada_gpus + hopper_gpus)
            
            if is_compatible:
                compatibility_report['gpu_compatible'] = True
                print("‚úÖ GPU Compatible avec Flash Attention")
            else:
                print("‚ö†Ô∏è GPU peut avoir des limitations")
                compatibility_report['gpu_compatible'] = True  # Tentative quand m√™me
        
        # System Dependencies
        deps_available = self.check_build_dependencies()
        
        # Verdict final
        compatibility_report['system_ready'] = all([
            compatibility_report['cuda_compatible'],
            compatibility_report['pytorch_compatible'], 
            compatibility_report['gpu_compatible'],
            deps_available
        ])
        
        print(f"\nüéØ Compatibilit√© Syst√®me: {'‚úÖ PR√äT' if compatibility_report['system_ready'] else '‚ùå PROBL√àMES'}")
        
        # üî¨ NOUVEAU: Sauvegarde rapport compatibilit√©
        compat_path = f"{self.results_dir}/environment/compatibility_report.json"
        with open(compat_path, 'w') as f:
            json.dump(compatibility_report, f, indent=4, default=str)
        
        return compatibility_report
    
    def check_build_dependencies(self):
        """V√©rification d√©pendances build avec logging"""
        print("\nüîß V√âRIFICATION D√âPENDANCES BUILD")
        print("-" * 40)
        
        required_deps = ['packaging', 'ninja']
        missing_deps = []
        deps_status = {}
        
        for dep in required_deps:
            try:
                __import__(dep)
                print(f"‚úÖ {dep}: Disponible")
                deps_status[dep] = {'available': True, 'version': 'detected'}
            except ImportError:
                print(f"‚ùå {dep}: Manquant")
                missing_deps.append(dep)
                deps_status[dep] = {'available': False, 'error': 'ImportError'}
        
        # üî¨ NOUVEAU: Log d√©taill√© des d√©pendances
        deps_log = {
            'timestamp': datetime.now().isoformat(),
            'required_dependencies': required_deps,
            'missing_dependencies': missing_deps,
            'dependencies_status': deps_status,
            'installation_attempted': False
        }
        
        if missing_deps:
            print(f"\nüì¶ Installation d√©pendances manquantes...")
            deps_log['installation_attempted'] = True
            
            try:
                start_time = time.time()
                result = subprocess.run([sys.executable, '-m', 'pip', 'install'] + missing_deps, 
                                      check=True, capture_output=True, text=True)
                install_time = time.time() - start_time
                
                deps_log['installation_success'] = True
                deps_log['installation_time_seconds'] = install_time
                deps_log['installation_output'] = result.stdout
                
                print("‚úÖ D√©pendances install√©es")
                return True
                
            except subprocess.CalledProcessError as e:
                deps_log['installation_success'] = False
                deps_log['installation_error'] = str(e)
                deps_log['installation_stderr'] = e.stderr
                
                print(f"‚ùå Erreur installation: {e}")
                return False
        
        # Sauvegarde log d√©pendances
        deps_path = f"{self.results_dir}/environment/dependencies_log.json"
        with open(deps_path, 'w') as f:
            json.dump(deps_log, f, indent=4)
        
        return True
    
    def install_flash_attention_with_monitoring(self):
        """Installation Flash Attention avec monitoring temps r√©el"""
        print("\nüöÄ INSTALLATION FLASH ATTENTION AVEC MONITORING")
        print("=" * 70)
        
        # üî¨ NOUVEAU: D√©marrage monitoring pendant installation
        self.monitor.start_continuous_monitoring()
        
        start_time = time.time()
        installation_log = {
            'start_time': datetime.now().isoformat(),
            'attempts': [],
            'final_success': False
        }
        
        # M√©thodes d'installation
        install_methods = [
            {
                'name': 'Official Pip Install',
                'command': [sys.executable, '-m', 'pip', 'install', 'flash-attn', '--no-build-isolation'],
                'timeout': 1800
            },
            {
                'name': 'Specific Version 2.7.3',
                'command': [sys.executable, '-m', 'pip', 'install', 'flash-attn==2.7.3', '--no-build-isolation'],
                'timeout': 1800
            }
        ]
        
        for method in install_methods:
            print(f"\nüîÑ Tentative: {method['name']}")
            print(f"üìù Commande: {' '.join(method['command'])}")
            
            attempt_start = time.time()
            attempt_log = {
                'method': method['name'],
                'command': ' '.join(method['command']),
                'start_time': datetime.now().isoformat(),
                'success': False
            }
            
            try:
                result = subprocess.run(
                    method['command'],
                    capture_output=True,
                    text=True,
                    timeout=method['timeout']
                )
                
                attempt_duration = time.time() - attempt_start
                attempt_log['duration_seconds'] = attempt_duration
                attempt_log['return_code'] = result.returncode
                attempt_log['stdout'] = result.stdout[:1000]  # Limiter taille
                attempt_log['stderr'] = result.stderr[:1000]
                
                if result.returncode == 0:
                    print(f"‚úÖ Installation r√©ussie! ({attempt_duration:.1f}s)")
                    attempt_log['success'] = True
                    
                    # Test import
                    if self.test_flash_attention_import():
                        self.installation_success = True
                        total_duration = time.time() - start_time
                        
                        installation_log['final_success'] = True
                        installation_log['total_duration_seconds'] = total_duration
                        installation_log['successful_method'] = method['name']
                        
                        print(f"üéâ Flash Attention pr√™t! (Total: {total_duration:.1f}s)")
                        break
                    else:
                        print("‚ùå Installation r√©ussie mais import √©choue")
                        attempt_log['import_failed'] = True
                else:
                    print(f"‚ùå √âchec installation: Code {result.returncode}")
                    
            except subprocess.TimeoutExpired:
                print(f"‚è±Ô∏è Timeout apr√®s {method['timeout']}s")
                attempt_log['timeout'] = True
                attempt_log['timeout_seconds'] = method['timeout']
            except Exception as e:
                print(f"‚ùå Erreur: {e}")
                attempt_log['exception'] = str(e)
            
            installation_log['attempts'].append(attempt_log)
        
        # üî¨ NOUVEAU: Capture √©tat syst√®me post-installation
        post_install_state = self.monitor.get_current_system_state()
        installation_log['post_install_system_state'] = post_install_state
        
        # Sauvegarde log installation
        install_path = f"{self.results_dir}/environment/installation_log.json"
        with open(install_path, 'w') as f:
            json.dump(installation_log, f, indent=4, default=str)
        
        if not self.installation_success:
            total_duration = time.time() - start_time
            installation_log['total_duration_seconds'] = total_duration
            print(f"\n‚ùå Installation Flash Attention √©chou√©e (Total: {total_duration:.1f}s)")
        
        return self.installation_success
    
    def test_flash_attention_import(self):
        """Test import Flash Attention avec d√©tails"""
        import_log = {
            'timestamp': datetime.now().isoformat(),
            'import_attempts': []
        }
        
        try:
            # Test import principal
            import flash_attn
            import_log['flash_attn_imported'] = True
            
            # Version
            version = getattr(flash_attn, '__version__', 'Unknown')
            import_log['version'] = version
            print(f"üìä Version Flash Attention: {version}")
            
            # Test import fonction
            from flash_attn import flash_attn_func
            import_log['flash_attn_func_imported'] = True
            print("‚úÖ Import Flash Attention: OK")
            
            # üî¨ NOUVEAU: Test fonctionnel basique
            try:
                if torch.cuda.is_available():
                    # Test minimal fonction
                    batch, heads, seq, dim = 1, 4, 64, 32
                    q = torch.randn(batch, seq, heads, dim, device='cuda', dtype=torch.float16)
                    k = torch.randn(batch, seq, heads, dim, device='cuda', dtype=torch.float16)
                    v = torch.randn(batch, seq, heads, dim, device='cuda', dtype=torch.float16)
                    
                    with torch.no_grad():
                        output = flash_attn_func(q, k, v)
                    
                    import_log['functional_test'] = {
                        'success': True,
                        'input_shape': [batch, seq, heads, dim],
                        'output_shape': list(output.shape),
                        'device': 'cuda',
                        'dtype': 'float16'
                    }
                    print("üß™ Test fonctionnel Flash Attention: ‚úÖ R√âUSSI")
                    
                    # Nettoyage
                    del q, k, v, output
                    torch.cuda.empty_cache()
                    
            except Exception as e:
                import_log['functional_test'] = {
                    'success': False,
                    'error': str(e)
                }
                print(f"‚ö†Ô∏è Test fonctionnel √©chou√©: {e}")
            
            # Sauvegarde log import
            import_path = f"{self.results_dir}/environment/import_test_log.json"
            with open(import_path, 'w') as f:
                json.dump(import_log, f, indent=4, default=str)
            
            return True
            
        except ImportError as e:
            import_log['import_error'] = str(e)
            import_log['success'] = False
            print(f"‚ùå Import Flash Attention: {e}")
            
            # Sauvegarde log import √©chec
            import_path = f"{self.results_dir}/environment/import_test_log.json"
            with open(import_path, 'w') as f:
                json.dump(import_log, f, indent=4, default=str)
            
            return False
    
    def run_monitored_training(self, config):
        """Entra√Ænement YOLO12 avec monitoring exhaustif"""
        print("\nüéØ ENTRA√éNEMENT YOLO12 + FLASH ATTENTION + MONITORING COMPLET")
        print("=" * 80)
        
        if not self.installation_success:
            print("‚ùå Flash Attention non disponible - Abandon")
            return None
        
        # üî¨ NOUVEAU: Configuration monitoring training
        training_log = {
            'start_time': datetime.now().isoformat(),
            'config': config.copy(),
            'monitoring_active': True,
            'epochs_completed': 0,
            'training_success': False
        }
        
        try:
            # Configuration Flash Attention
            flash_config = config.copy()
            flash_config['project'] = self.results_dir
            flash_config['name'] = 'yolo12_flash_attention_monitored'
            
            # Variables d'environnement Flash Attention
            os.environ['FLASH_ATTENTION_SKIP_CUDA_BUILD'] = '0'
            os.environ['FLASH_ATTENTION_FORCE_BUILD'] = '1'
            
            print("üìã Configuration Flash Attention compl√®te:")
            for key, value in flash_config.items():
                print(f"   {key}: {value}")
            
            # üî¨ NOUVEAU: Snapshot pr√©-training
            pre_training_state = {
                'timestamp': datetime.now().isoformat(),
                'system_state': self.monitor.get_current_system_state(),
                'gpu_memory_before': torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0,
                'config_validated': True
            }
            
            training_log['pre_training_state'] = pre_training_state
            
            # Chargement mod√®le avec monitoring
            print(f"\nüîÑ Chargement YOLO12: {config['model']}")
            load_start = time.time()
            
            model = YOLO(config['model'])
            
            load_duration = time.time() - load_start
            post_load_state = self.monitor.get_current_system_state()
            
            training_log['model_loading'] = {
                'duration_seconds': load_duration,
                'post_load_gpu_memory': torch.cuda.memory_allocated(0) / 1e9 if torch.cuda.is_available() else 0,
                'post_load_system_state': post_load_state
            }
            
            print(f"‚úÖ Mod√®le charg√© en {load_duration:.2f}s")
            
            # üî¨ NOUVEAU: Hook monitoring epochs
            class TrainingMonitorHook:
                def __init__(self, monitor, training_log):
                    self.monitor = monitor
                    self.training_log = training_log
                    self.epoch_count = 0
                
                def on_train_epoch_end(self, trainer):
                    self.epoch_count += 1
                    
                    # Extraction m√©triques YOLO
                    epoch_metrics = {
                        'epoch': self.epoch_count,
                        'timestamp': datetime.now().isoformat(),
                        'losses': {
                            'box_loss': float(getattr(trainer.loss_items, 0, 0)) if hasattr(trainer, 'loss_items') else 0,
                            'cls_loss': float(getattr(trainer.loss_items, 1, 0)) if hasattr(trainer, 'loss_items') else 0,
                            'dfl_loss': float(getattr(trainer.loss_items, 2, 0)) if hasattr(trainer, 'loss_items') else 0
                        },
                        'learning_rate': float(getattr(trainer.optimizer, 'param_groups', [{}])[0].get('lr', 0)),
                        'system_state': self.monitor.get_current_system_state()
                    }
                    
                    # Log √©poque avec monitoring complet
                    self.monitor.log_training_epoch(self.epoch_count, epoch_metrics)
                    self.training_log['epochs_completed'] = self.epoch_count
                    
                    print(f"üìä √âpoque {self.epoch_count} - Syst√®me monitor√©")
            
            # Installation hook (si support√© par ultralytics)
            hook = TrainingMonitorHook(self.monitor, training_log)
            
            print("üöÄ D√©but entra√Ænement avec monitoring exhaustif...")
            training_start = time.time()
            
            # üî¨ NOUVEAU: Monitoring pr√©-training final
            pre_train_snapshot = self.monitor.get_current_system_state()
            training_log['immediate_pre_training'] = pre_train_snapshot
            
            # ENTRA√éNEMENT PRINCIPAL
            results = model.train(**flash_config)
            
            training_duration = time.time() - training_start
            
            # üî¨ NOUVEAU: Capture r√©sultats complets
            training_log['training_success'] = True
            training_log['total_training_duration_seconds'] = training_duration
            training_log['total_training_duration_hours'] = training_duration / 3600
            training_log['end_time'] = datetime.now().isoformat()
            
            # M√©triques finales si disponibles
            if hasattr(results, 'results_dict'):
                training_log['final_metrics'] = results.results_dict
            
            # √âtat syst√®me final
            final_state = self.monitor.get_current_system_state()
            training_log['final_system_state'] = final_state
            
            print(f"‚úÖ Entra√Ænement termin√©! ({training_duration/3600:.2f}h)")
            
            # üî¨ NOUVEAU: Validation post-training
            try:
                validation_start = time.time()
                val_results = model.val()
                validation_duration = time.time() - validation_start
                
                training_log['validation'] = {
                    'success': True,
                    'duration_seconds': validation_duration,
                    'results': getattr(val_results, 'results_dict', {}) if hasattr(val_results, 'results_dict') else {}
                }
                
                print(f"‚úÖ Validation termin√©e ({validation_duration:.2f}s)")
                
            except Exception as e:
                training_log['validation'] = {
                    'success': False,
                    'error': str(e)
                }
                print(f"‚ö†Ô∏è Validation √©chou√©e: {e}")
            
            # Sauvegarde log training complet
            self.save_comprehensive_results(training_log, results)
            
            return results
            
        except Exception as e:
            # üî¨ NOUVEAU: Log erreur compl√®te
            training_log['training_success'] = False
            training_log['error'] = str(e)
            training_log['error_time'] = datetime.now().isoformat()
            training_log['error_system_state'] = self.monitor.get_current_system_state()
            
            print(f"‚ùå Erreur entra√Ænement Flash Attention: {e}")
            
            # Sauvegarde m√™me en cas d'erreur
            self.save_comprehensive_results(training_log, None)
            
            return None
    
    def save_comprehensive_results(self, training_log, results):
        """Sauvegarde r√©sultats exhaustifs"""
        print("\nüíæ SAUVEGARDE R√âSULTATS EXHAUSTIFS...")
        
        # 1. Log training principal
        training_path = f"{self.results_dir}/training_logs/complete_training_log.json"
        with open(training_path, 'w') as f:
            json.dump(training_log, f, indent=4, default=str)
        
        # 2. R√©sultats YOLO si disponibles
        if results:
            try:
                results_data = {
                    'timestamp': datetime.now().isoformat(),
                    'save_dir': str(results.save_dir) if hasattr(results, 'save_dir') else None,
                    'results_dict': getattr(results, 'results_dict', {}),
                    'metrics': {}
                }
                
                # Extraction m√©triques si disponibles
                if hasattr(results, 'results_dict'):
                    results_data['metrics'] = results.results_dict
                
                results_path = f"{self.results_dir}/training_logs/yolo_results.json"
                with open(results_path, 'w') as f:
                    json.dump(results_data, f, indent=4, default=str)
                    
            except Exception as e:
                error_path = f"{self.results_dir}/training_logs/results_extraction_error.json"
                with open(error_path, 'w') as f:
                    json.dump({'error': str(e), 'timestamp': datetime.now().isoformat()}, f, indent=4)
        
        # 3. Arr√™t monitoring et sauvegarde finale
        self.monitor.stop_monitoring()
        
        print(f"‚úÖ R√©sultats exhaustifs sauvegard√©s dans: {self.results_dir}")

def main_comprehensive_flash_experiment():
    """Exp√©rience Flash Attention avec monitoring exhaustif"""
    
    print("üî¨ EXP√âRIENCE FLASH ATTENTION AVEC MONITORING EXHAUSTIF")
    print("üéØ Comparaison Scientifique Compl√®te vs SDPA")
    print("üá®üá© Kennedy Kitoko - Agricultural AI Innovation")
    print("=" * 80)
    
    # Initialisation exp√©rience avanc√©e
    experiment = AdvancedFlashAttentionExperiment()
    
    # Phase 1: V√©rification + capture environnement
    print("\n" + "="*50)
    print("PHASE 1: V√âRIFICATION & ENVIRONNEMENT")
    print("="*50)
    
    compatibility = experiment.verify_system_compatibility()
    
    if not compatibility['system_ready']:
        print("\n‚ùå Syst√®me non compatible")
        return False
    
    # Phase 2: Installation avec monitoring
    print("\n" + "="*50)
    print("PHASE 2: INSTALLATION AVEC MONITORING")
    print("="*50)
    
    installation_success = experiment.install_flash_attention_with_monitoring()
    
    if not installation_success:
        print("\n‚ö†Ô∏è Installation Flash Attention √©chou√©e")
        print("üìä Donn√©es d'installation disponibles pour analyse")
    
    # Phase 3: Configuration identique SDPA
    print("\n" + "="*50)
    print("PHASE 3: CONFIGURATION IDENTIQUE SDPA")
    print("="*50)
    
    config = {
        'model': '/mnt/c/Users/kitok/Desktop/SmartFarm_OS/IA/YOLO12_FLASH_attn/yolo12n.pt',
        'data': '/mnt/c/Users/kitok/Desktop/SmartFarm_OS/IA/YOLO12_FLASH_attn/Weeds-3/data.yaml',
        'epochs': 100,
        'batch': 8,
        'imgsz': 640,
        'device': 'cuda:0',
        'workers': 6,
        'optimizer': 'AdamW',
        'lr0': 0.001,
        'amp': True,
        'cache': 'ram',
        'patience': 30,
        'save_period': 5
    }
    
    print("üìã Configuration valid√©e - Identique SDPA:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    
    # Phase 4: Entra√Ænement avec monitoring exhaustif
    print("\n" + "="*50)
    print("PHASE 4: ENTRA√éNEMENT AVEC MONITORING EXHAUSTIF")
    print("="*50)
    
    if installation_success:
        flash_results = experiment.run_monitored_training(config)
        
        if flash_results:
            print("\nüéâ ENTRA√éNEMENT FLASH ATTENTION R√âUSSI!")
            print("üìä Toutes les m√©triques captur√©es")
        else:
            print("\n‚ö†Ô∏è Entra√Ænement √©chou√© mais donn√©es collect√©es")
    else:
        print("\nüìö Flash Attention non disponible")
        print("üíæ Donn√©es d'environnement et tentatives sauvegard√©es")
    
    # Phase 5: G√©n√©ration rapport comparatif
    print("\n" + "="*50)
    print("PHASE 5: RAPPORT COMPARATIF FINAL")
    print("="*50)
    
    print(f"üìÅ Toutes les donn√©es sauvegard√©es dans:")
    print(f"   {experiment.results_dir}")
    print("\nüìä Structure des donn√©es g√©n√©r√©es:")
    print("   ‚îú‚îÄ‚îÄ environment/ (environnement syst√®me complet)")
    print("   ‚îú‚îÄ‚îÄ training_logs/ (m√©triques par √©poque)")
    print("   ‚îú‚îÄ‚îÄ resource_monitoring/ (CPU, GPU, RAM en temps r√©el)")
    print("   ‚îú‚îÄ‚îÄ system_info/ (snapshots syst√®me)")
    print("   ‚îî‚îÄ‚îÄ comparison_data/ (donn√©es pour comparaison)")
    
    print("\n‚úÖ EXP√âRIENCE EXHAUSTIVE TERMIN√âE!")
    print("üî¨ Donn√©es scientifiques compl√®tes disponibles")
    print("üìä Comparaison SDPA vs Flash Attention pr√™te")
    
    return True

if __name__ == "__main__":
    success = main_comprehensive_flash_experiment()
    
    if success:
        print("\nüèÜ SUCC√àS: Donn√©es exhaustives collect√©es!")
        print("üìà Pr√™t pour analyse comparative scientifique")
    else:
        print("\n‚ö†Ô∏è Exp√©rience partielle mais donn√©es disponibles")
        print("üîç V√©rifiez les logs pour diagnostic")